# LLM Output Similarity Experiment

**v1.0** — A bare-bones local experiment that sends the same set of prompts to multiple LLMs and measures how similar their outputs are using semantic cosine similarity of local embeddings.

---

## Overview

This project compares responses generated by several locally-hosted LLMs (served via [Ollama](https://ollama.com)) across a configurable set of prompts. Responses are normalized, embedded with a local embedding model, and compared pairwise to produce aggregate similarity heatmaps.

### Milestones

| # | Scope |
|---|-------|
| M1 | Inputs / config / prompt loading / normalization |
| M2 | Unified HTTP client + inference loop + `responses_df` |
| M3 | Embeddings → cosine similarity → aggregate heatmap |

---

## Project Structure

```
llm_prompting_experiment/
├── llm_prompting_experiment.ipynb  # Main experiment notebook
├── llm_models_json.json            # Model configurations (generators + embedder)
├── prompts.txt                     # Primary prompt set (entries separated by ---)
├── prompts2.txt                    # Secondary prompt set
├── system_prompt.txt               # System prompt applied to all models
├── check_ollama_config.py          # Utility: verify Ollama server and model availability
├── requirements.txt                # Python dependencies
└── specs_folder/
    └── prompt_exp.json             # Project specification and functional requirements
```

---

## Prerequisites

### 1. Install Ollama

Download and install Ollama from [https://ollama.com](https://ollama.com), then start the server:

```bash
ollama serve
```

### 2. Pull the required models

Generator models:

```bash
ollama pull qwen2.5:1.5b-instruct
ollama pull qwen3:1.7b
ollama pull qwen:1.8b
ollama pull qwen2.5:3b-instruct
ollama pull qwen3:4b
ollama pull qwen2.5:7b-instruct
```

Embedding model:

```bash
ollama pull nomic-embed-text
```

### 3. Set up the Python environment

Install the required packages:

```bash
pip install -r requirements.txt
```

### 4. Configure the experiment

- **Models**: Edit `llm_models_json.json` to add, remove, or reconfigure models. Each entry requires `kind`, `provider`, `model`, `base_url`, and `api_key`.
- **Prompts**: Edit `prompts.txt` to define the prompts to test. Separate each prompt with a line containing only `---`.
- **System prompt**: Edit `system_prompt.txt` with the system prompt to apply to all model calls.
- **Environment variables** (optional): Create a `.env` file to override defaults such as `SYSTEM_PROMPT_FILE`, `MAX_SYSTEM_LINES`, and `MAX_SYSTEM_TOKENS_APPROX`.

---

## Usage

### Verify Ollama setup

Before running the notebook, confirm the Ollama server is running and the required models are available:

```bash
python check_ollama_config.py
```

### Run the notebook

Open `llm_prompting_experiment.ipynb` in VS Code (or Jupyter Lab/Notebook) and run all cells in order.

The notebook will:

1. Load model configs and validate them.
2. Load and truncate the system prompt.
3. Parse and normalize prompts from `prompts.txt`.
4. Call each generator model for every prompt via a unified HTTP client.
5. Store all raw and normalized responses in `responses_df`.
6. Compute local embeddings for each response using `nomic-embed-text`.
7. Compute pairwise cosine similarity between model responses per prompt.
8. Render aggregate similarity heatmaps and per-prompt comparisons.
9. Provide interactive dropdowns to inspect individual prompt/model responses.

---

## Model Configuration Format

`llm_models_json.json` maps an alias to a configuration dict:

```json
{
  "my-model-alias": {
    "kind": "generator",
    "provider": "openai_compatible",
    "model": "model-name-in-ollama",
    "base_url": "http://localhost:11434",
    "api_key": "ollama"
  }
}
```

| Field | Values | Description |
|-------|--------|-------------|
| `kind` | `generator`, `embedder`, `judge` | Role of the model in the pipeline |
| `provider` | `openai_compatible` | API interface to use |
| `model` | string | Model name as known to Ollama |
| `base_url` | URL | Ollama server base URL |
| `api_key` | string | Pass `"ollama"` for local Ollama |

---

## Prompt File Format

Prompts in `prompts.txt` are plain text, separated by a line containing only `---`:

```
First prompt text here.
---
Second prompt text here.
---
Third prompt text here.
```

---

## Dependencies

| Package | Purpose |
|---------|---------|
| `httpx` | Async-capable HTTP client for model API calls |
| `requests` | HTTP calls in utility scripts |
| `numpy` | Numerical operations and cosine similarity |
| `pandas` | Tabular storage of responses (`responses_df`) |
| `matplotlib` | Heatmap and similarity visualizations |
| `ipywidgets` | Interactive dropdowns for result exploration |
| `python-dotenv` | Environment variable loading from `.env` |

---

## Limitations (v1.0)

- No result caching or experiment tracking.
- No export to CSV/JSON.
- No Hugging Face NLI/embedding model support (local Ollama only).
- Aggregate heatmaps only; no statistical significance testing.
