{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e301c3c",
   "metadata": {},
   "source": [
    "# LLM Output Similarity Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c75dc27",
   "metadata": {},
   "source": [
    "## 1. Introduction - Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d75a12",
   "metadata": {},
   "source": [
    "**v1.0** — Bare-bones comparison of LLM outputs using two aggregate similarity metrics:\n",
    "- **Cosine similarity** of local embeddings (semantic).\n",
    "\n",
    "### Prerequisites\n",
    "1. Copy `.env.example` → `.env` and fill in your model configs.\n",
    "2. Edit `prompts.txt` (separate entries with a line containing only `---`).\n",
    "3. Edit `system_prompt.txt` with your desired system prompt.\n",
    "4. Ensure all referenced Ollama models are pulled: `ollama pull <model>`.\n",
    "\n",
    "### Milestones\n",
    "| # | Scope |\n",
    "|---|-------|\n",
    "| M1 | Inputs / config / prompt loading / normalization |\n",
    "| M2 | Unified HTTP client + inference loop + `responses_df` |\n",
    "| M3 | Embeddings → cosine similarity → aggregate heatmap |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cacd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Imports ────────────────────────────────────────────────────────────────────\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "from itertools import combinations\n",
    "\n",
    "import httpx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc947ff",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Inputs, Configuration & Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a688dd2",
   "metadata": {},
   "source": [
    "### 2.1 Configuration and Models Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f72320",
   "metadata": {},
   "source": [
    "Note that models are not actually loaded, only their configuration read. Models will be cold loaded for each separate prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bced07cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── FR-1 / FR-2: Load model configs from .env ─────────────────────────────────\n",
    "load_dotenv()\n",
    "\n",
    "# Read the JSON string from the json file and parse it into a dictionary\n",
    "with open(\"llm_models_json.json\", \"r\") as f:\n",
    "    _models_json_raw = f.read()\n",
    "print(_models_json_raw)  # Debug: print raw JSON string to verify it's loaded correctly\n",
    "MODELS: dict[str, dict] = json.loads(_models_json_raw)\n",
    "\n",
    "# Validate each alias\n",
    "# NFR-1: Ensure all required keys are present and valid\n",
    "_REQUIRED_KEYS = {\"kind\", \"provider\", \"model\", \"base_url\", \"api_key\"}\n",
    "# Kinds: generator = text generation, embedder = vector embedding, judge = evaluation/scoring\n",
    "_VALID_KINDS   = {\"generator\", \"embedder\", \"judge\"}\n",
    "\n",
    "# Report any missing keys or invalid kinds before proceeding\n",
    "for _alias, _cfg in MODELS.items():\n",
    "    _missing = _REQUIRED_KEYS - _cfg.keys()\n",
    "    assert not _missing, f\"Model '{_alias}' is missing required keys: {_missing}\"\n",
    "    assert _cfg[\"kind\"] in _VALID_KINDS, (\n",
    "        f\"Model '{_alias}' has invalid kind '{_cfg['kind']}'. Must be one of {_VALID_KINDS}.\"\n",
    "    )\n",
    "\n",
    "# NFR-1: Show redacted config (never print api_key)\n",
    "print(f\"Loaded {len(MODELS)} model(s):\")\n",
    "for _alias, _cfg in MODELS.items():\n",
    "    _safe = {k: (\"***\" if k == \"api_key\" else v) for k, v in _cfg.items()}\n",
    "    print(f\"  {_alias}: {_safe}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ed41ca",
   "metadata": {},
   "source": [
    "### 2.2 System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00441bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── FR-3: Load system prompt with max-lines and approx max-tokens enforcement ──\n",
    "SYSTEM_PROMPT_FILE      = os.environ.get(\"SYSTEM_PROMPT_FILE\",      \"system_prompt.txt\")\n",
    "MAX_SYSTEM_LINES        = int(os.environ.get(\"MAX_SYSTEM_LINES\",        \"100\"))\n",
    "MAX_SYSTEM_TOKENS_APPROX = int(os.environ.get(\"MAX_SYSTEM_TOKENS_APPROX\", \"1500\"))\n",
    "\n",
    "def load_system_prompt(path: str, max_lines: int, max_tokens_approx: int) -> str:\n",
    "    \"\"\"Load system prompt from *path*, truncating by line count and approx token count.\n",
    "    Args:\n",
    "        path: Path to the system prompt file.\n",
    "        max_lines: Maximum number of lines to read.\n",
    "        max_tokens_approx: Approximate maximum number of tokens.\n",
    "    Returns:\n",
    "        The loaded system prompt as a string, truncated according to the specified limits.\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as fh:\n",
    "        lines = fh.readlines()\n",
    "\n",
    "    # Warn of truncation if file exceeds line limit\n",
    "    if len(lines) > max_lines:\n",
    "        print(\n",
    "            f\"Warning: System prompt has {len(lines)} lines, exceeding the max of {max_lines}. \"\n",
    "            f\"Truncating to the first {max_lines} lines.\"\n",
    "        )\n",
    "\n",
    "    # Hard line-count cap\n",
    "    lines = lines[:max_lines]\n",
    "    text  = \"\".join(lines)\n",
    "\n",
    "    # Warn of truncation if text exceeds approx token limit\n",
    "    if len(text) > max_tokens_approx * 4:\n",
    "        print(\n",
    "            f\"Warning: System prompt has approximately {len(text) // 4} tokens, \"\n",
    "            f\"exceeding the approx max of {max_tokens_approx}. \"\n",
    "            f\"Truncating to the first {max_tokens_approx * 4} characters.\"\n",
    "        )\n",
    "\n",
    "    # Heuristic: ~4 chars per token\n",
    "    max_chars = max_tokens_approx * 4\n",
    "    if len(text) > max_chars:\n",
    "        text = text[:max_chars]\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# Load the system prompt with the specified limits\n",
    "system_prompt = load_system_prompt(\n",
    "    SYSTEM_PROMPT_FILE, MAX_SYSTEM_LINES, MAX_SYSTEM_TOKENS_APPROX\n",
    ")\n",
    "print(f\"System prompt ({len(system_prompt)} chars):\")\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63375f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── FR-2 (cont.): Load prompts from .txt split by delimiter '---' ──────────────\n",
    "PROMPTS_FILE = os.environ.get(\"PROMPTS_FILE\", \"prompts.txt\")\n",
    "DELIMITER    = \"---\"\n",
    "\n",
    "with open(PROMPTS_FILE, \"r\", encoding=\"utf-8\") as prompts_file:\n",
    "    _raw_prompts = prompts_file.read()\n",
    "\n",
    "# Split on lines that contain *only* the delimiter (strip surrounding whitespace)\n",
    "_raw_parts = re.split(r\"(?m)^---\\s*$\", _raw_prompts)\n",
    "prompts: list[str] = [p.strip() for p in _raw_parts if p.strip()]\n",
    "\n",
    "print(f\"Loaded {len(prompts)} prompt(s):\")\n",
    "for _i, _p in enumerate(prompts):\n",
    "    _preview = _p[:120].replace(\"\\n\", \" \")\n",
    "    print(f\"  [prompt_{_i}] {_preview}{'...' if len(_p) > 120 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae6b8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Output normalisation (applied before embeddings + NLI) ────────────────────\n",
    "def normalize_response(text: str) -> str:\n",
    "    \"\"\"Normalize LLM output for downstream similarity scoring.\n",
    "\n",
    "    Steps (in order):\n",
    "    1. Strip leading/trailing whitespace.\n",
    "    2. Normalize line endings to \\\\n.\n",
    "    3. Remove standalone markdown fence lines (e.g. ```python) — *keep* content.\n",
    "    4. Collapse multiple spaces/tabs → single space (preserving newlines).\n",
    "    5. Collapse 3+ consecutive newlines → 2 newlines.\n",
    "    6. Final strip.\n",
    "\n",
    "    Args:\n",
    "        text: The raw text output from the LLM.\n",
    "    Returns:\n",
    "        The normalized text, cleaned according to the steps above.\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    # Step 2: normalize line endings\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    # Step 3: remove standalone fence lines (``` or ```lang) — keep code content\n",
    "    text = re.sub(r\"(?m)^```[a-zA-Z0-9_+-]*\\s*$\", \"\", text)\n",
    "    # Step 4: collapse multiple spaces / tabs (not newlines)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    # Step 5: collapse 3+ newlines → 2\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# Smoke test\n",
    "_sample = \"  Hello  \\r\\n\\n\\n\\n```python\\nprint('hi')\\n```\\n  World  \"\n",
    "print(repr(normalize_response(_sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd2878b",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Unified OpenAI-Compatible Client & Inference Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0514043a",
   "metadata": {},
   "source": [
    "### 3.1 Client Class Declaration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33778c97",
   "metadata": {},
   "source": [
    "We create a generalized wrapper class around the OpenAI-compatible HTTP API, with which we will be calling our models regardless of role in the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b73e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── FR-4: Unified OpenAI-compatible HTTP client ────────────────────────────────\n",
    "_MAX_RETRIES          = 3\n",
    "_RETRY_STATUSES       = {429, 500, 502, 503, 504}\n",
    "_BASE_BACKOFF_S       = 1.0    # seconds\n",
    "_REQUEST_TIMEOUT      = 120.0  # seconds — used for non-streaming calls (embed, etc.)\n",
    "_STREAM_TTFT_TIMEOUT  = 90.0   # seconds — time to first token (model may need to load weights)\n",
    "_STREAM_TOKEN_TIMEOUT = 30.0   # seconds — max wait between tokens once streaming has started\n",
    "\n",
    "\n",
    "class LLMClient:\n",
    "    \"\"\"Thin wrapper around the OpenAI-compatible HTTP API.\n",
    "\n",
    "    All model-specific details (base_url, api_key, model name) are resolved\n",
    "    from the *models* config dict by alias. Retries with exponential backoff\n",
    "    are applied automatically on 429 / 5xx responses (NFR-2).\n",
    "    Args:\n",
    "    - models: A dict mapping model aliases to their config dicts, which must include:\n",
    "        - kind: \"generator\", \"embedder\", or \"judge\"\n",
    "        - provider: e.g. \"openai\", \"azure\", \"anthropic\"\n",
    "        - model: the model name to use in API calls\n",
    "        - base_url: the base URL for API calls (e.g. \"https://api.openai.com/v1\")\n",
    "        - api_key: the API key for authentication (never printed in logs)\n",
    "    Returns:\n",
    "    - An instance of LLMClient that can be used to make chat and embedding requests.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, models: dict[str, dict]) -> None:\n",
    "        self._models = models\n",
    "\n",
    "    # ── public interface ──────────────────────────────────────────────────────\n",
    "\n",
    "    def chat(\n",
    "        self,\n",
    "        alias: str,\n",
    "        messages: list[dict],\n",
    "        **params,\n",
    "    ) -> dict:\n",
    "        \"\"\"POST /v1/chat/completions for *alias* with the given *messages*.\n",
    "        Args:\n",
    "            alias: The model alias to use (must be defined in the config).\n",
    "            messages: The list of message dicts to send in the chat completion request.\n",
    "            **params: Additional parameters to include in the request body (e.g. temperature).\n",
    "        Returns:\n",
    "            The JSON response from the API as a dict.\n",
    "        \"\"\"\n",
    "        cfg = self._get_cfg(alias)\n",
    "        url = cfg[\"base_url\"].rstrip(\"/\") + \"/v1/chat/completions\"\n",
    "        body: dict = {\n",
    "            \"model\":       cfg[\"model\"],\n",
    "            \"messages\":    messages,\n",
    "            \"temperature\": 0,        # NFR-3: deterministic by default\n",
    "            \"stream\":      True,     # use streaming to detect generation progress\n",
    "            **params,\n",
    "        }\n",
    "        body.update(cfg.get(\"extra\") or {})\n",
    "        return self._post_with_retry(url, self._headers(cfg), body)\n",
    "\n",
    "    def embed(\n",
    "        self,\n",
    "        alias: str,\n",
    "        texts: list[str],\n",
    "    ) -> list[list[float]]:\n",
    "        \"\"\"POST /v1/embeddings for *alias*, returning a list of embedding vectors.\n",
    "        Args:\n",
    "            alias: The model alias to use (must be defined in the config).\n",
    "            texts: The list of input texts to embed.\n",
    "        Returns:\n",
    "            A list of embedding vectors (one per input text).\"\"\"\n",
    "        cfg = self._get_cfg(alias)\n",
    "        url = cfg[\"base_url\"].rstrip(\"/\") + \"/v1/embeddings\"\n",
    "        body: dict = {\"model\": cfg[\"model\"], \"input\": texts}\n",
    "        resp = self._post_with_retry(url, self._headers(cfg), body)\n",
    "        return [item[\"embedding\"] for item in resp[\"data\"]]\n",
    "\n",
    "    # ── private helpers ───────────────────────────────────────────────────────\n",
    "\n",
    "    def _get_cfg(self, alias: str) -> dict:\n",
    "        \"\"\"Retrieve the configuration for a given model alias.\n",
    "        Args:\n",
    "            alias: The model alias to look up.\n",
    "        Returns:\n",
    "            The configuration dictionary for the specified model alias.\n",
    "        Raises:\n",
    "            KeyError: If the alias is not found in the models dictionary.\n",
    "        \"\"\"\n",
    "        if alias not in self._models:\n",
    "            raise KeyError(f\"Unknown model alias: '{alias}'. Known: {list(self._models)}\")\n",
    "        return self._models[alias]\n",
    "\n",
    "    @staticmethod\n",
    "    def _headers(cfg: dict) -> dict:\n",
    "        \"\"\"Generate headers for the API request.\n",
    "        Args:\n",
    "            cfg: The model configuration dictionary.\n",
    "        Returns:\n",
    "            A dictionary of headers for the API request.\n",
    "        \"\"\"\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        api_key = cfg.get(\"api_key\", \"\")\n",
    "        # Take care of no API key of local or hosted models (e.g. \"ollama\", \"none\", \"no_key_required\")\n",
    "        if api_key and api_key.lower() not in (\"\", \"none\", \"ollama\", \"no_key_required\"):\n",
    "            headers[\"Authorization\"] = f\"Bearer {api_key}\"\n",
    "        return headers\n",
    "\n",
    "    def _stream_response(self, http: httpx.Client, url: str, headers: dict, body: dict) -> dict:\n",
    "        \"\"\"Stream the response from the API, reassembling chunks into a standard response dict.\n",
    "\n",
    "        Uses a generous TTFT timeout for the first token (model may need to load weights),\n",
    "        then switches to a tighter inter-token timeout once generation is confirmed.\n",
    "        Args:\n",
    "            http: The httpx.Client instance to use for the request.\n",
    "            url: The URL to send the POST request to.\n",
    "            headers: The headers to include in the request.\n",
    "            body: The JSON body to include in the request.\n",
    "        Returns:\n",
    "            The JSON response from the API as a dict.\n",
    "        \"\"\"\n",
    "        full_content  = \"\"\n",
    "        finish_reason = None\n",
    "        first_token   = False\n",
    "\n",
    "        with http.stream(\n",
    "            \"POST\", url, headers=headers, json=body,\n",
    "            timeout=httpx.Timeout(\n",
    "                connect=10.0,\n",
    "                read=_STREAM_TTFT_TIMEOUT,   # generous wait for first token\n",
    "                write=10.0,\n",
    "                pool=5.0,\n",
    "            ),\n",
    "        ) as r:\n",
    "            r.raise_for_status()\n",
    "            for line in r.iter_lines():\n",
    "                if not line.startswith(\"data: \"):\n",
    "                    continue\n",
    "                data = line[len(\"data: \"):]\n",
    "                if data.strip() == \"[DONE]\":\n",
    "                    break\n",
    "                try:\n",
    "                    chunk = json.loads(data)\n",
    "                    delta = chunk[\"choices\"][0].get(\"delta\", {})\n",
    "                    token = delta.get(\"content\") or \"\"\n",
    "\n",
    "                    if token and not first_token:\n",
    "                        # Switch to the tighter inter-token timeout after first token arrives\n",
    "                        first_token = True\n",
    "                        r._timeout = httpx.Timeout(   # type: ignore[attr-defined]\n",
    "                            connect=10.0,\n",
    "                            read=_STREAM_TOKEN_TIMEOUT,\n",
    "                            write=10.0,\n",
    "                            pool=5.0,\n",
    "                        )\n",
    "\n",
    "                    full_content  += token\n",
    "                    finish_reason  = chunk[\"choices\"][0].get(\"finish_reason\")\n",
    "                except (json.JSONDecodeError, KeyError, IndexError):\n",
    "                    continue\n",
    "\n",
    "        return {\n",
    "            \"choices\": [{\n",
    "                \"message\":       {\"role\": \"assistant\", \"content\": full_content},\n",
    "                \"finish_reason\": finish_reason,\n",
    "            }]\n",
    "        }\n",
    "\n",
    "    def _post_with_retry(self, url: str, headers: dict, body: dict) -> dict:\n",
    "        \"\"\"POST request with retries on 429 / 5xx responses.\n",
    "\n",
    "        Delegates to _stream_response when body[\"stream\"] is True,\n",
    "        otherwise falls back to a standard blocking POST.\n",
    "        Args:\n",
    "            url: The URL to send the POST request to.\n",
    "            headers: The headers to include in the request.\n",
    "            body: The JSON body to include in the request.\n",
    "        Returns:\n",
    "            The JSON response from the API as a dict.\n",
    "        \"\"\"\n",
    "        backoff    = _BASE_BACKOFF_S\n",
    "        last_exc: Exception | None = None\n",
    "        is_stream  = body.get(\"stream\", False)\n",
    "\n",
    "        for attempt in range(_MAX_RETRIES):\n",
    "            try:\n",
    "                with httpx.Client(timeout=_REQUEST_TIMEOUT) as http:\n",
    "                    if is_stream:\n",
    "                        return self._stream_response(http, url, headers, body)\n",
    "\n",
    "                    r = http.post(url, headers=headers, json=body)\n",
    "\n",
    "                    if r.status_code in _RETRY_STATUSES and attempt < _MAX_RETRIES - 1:\n",
    "                        time.sleep(backoff)\n",
    "                        backoff *= 2\n",
    "                        continue\n",
    "\n",
    "                    r.raise_for_status()\n",
    "                    return r.json()\n",
    "\n",
    "            except httpx.HTTPStatusError as exc:\n",
    "                last_exc = exc\n",
    "                if exc.response.status_code not in _RETRY_STATUSES or attempt == _MAX_RETRIES - 1:\n",
    "                    raise\n",
    "                time.sleep(backoff)\n",
    "                backoff *= 2\n",
    "\n",
    "            except (httpx.TimeoutException, httpx.ConnectError) as exc:\n",
    "                last_exc = exc\n",
    "                if attempt == _MAX_RETRIES - 1:\n",
    "                    raise\n",
    "                time.sleep(backoff)\n",
    "                backoff *= 2\n",
    "\n",
    "        raise last_exc  # type: ignore[misc]\n",
    "\n",
    "\n",
    "client = LLMClient(MODELS)\n",
    "print(\"LLMClient ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5502d4c",
   "metadata": {},
   "source": [
    "### 3.2 Inference Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb87a3b1",
   "metadata": {},
   "source": [
    "We now use our previous defined wrapper class to loop over prompts x models and store the responses. Note that we iterate over models for each prompt separately, despite increased latency. This is intentional so that each response is cold started, and the model does not have any previous context when responding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf35bc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── FR-5 / FR-6 (responses_df): Run inference across all generator × prompts ──\n",
    "generator_aliases = [\n",
    "    alias for alias, cfg in MODELS.items() if cfg[\"kind\"] == \"generator\"\n",
    "]\n",
    "print(f\"Generator aliases: {generator_aliases}\")\n",
    "print(f\"Running {len(prompts)} prompt(s) × {len(generator_aliases)} model(s) …\\n\")\n",
    "\n",
    "records: list[dict] = []\n",
    "\n",
    "# We intentionally run each model cold for each prompt to avoid context\n",
    "# between prompts.\n",
    "for prompt_id, prompt_text in enumerate(prompts):\n",
    "    for alias in generator_aliases:\n",
    "        messages: list[dict] = []\n",
    "        if system_prompt:\n",
    "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt_text})\n",
    "\n",
    "        t0     = time.time()\n",
    "        status = \"ok\"\n",
    "        response_raw = \"\"\n",
    "        error: str | None = None\n",
    "\n",
    "        try:\n",
    "            resp         = client.chat(alias, messages)\n",
    "            response_raw = resp[\"choices\"][0][\"message\"][\"content\"]\n",
    "        except Exception as exc:  # noqa: BLE001\n",
    "            status = \"error\"\n",
    "            error  = str(exc)\n",
    "            print(f\"  [WARN] prompt_{prompt_id} / {alias} → {exc}\")\n",
    "\n",
    "        latency_ms    = round((time.time() - t0) * 1000, 1)\n",
    "        response_norm = normalize_response(response_raw) if status == \"ok\" else \"\"\n",
    "\n",
    "        records.append({\n",
    "            \"prompt_id\":     prompt_id,\n",
    "            \"model_alias\":   alias,\n",
    "            \"response_raw\":  response_raw,\n",
    "            \"response_norm\": response_norm,\n",
    "            \"status\":        status,\n",
    "            \"latency_ms\":    latency_ms,\n",
    "            \"created_at\":    datetime.now(timezone.utc).isoformat(),\n",
    "            \"error\":         error,\n",
    "        })\n",
    "        _icon = \"✓\" if status == \"ok\" else \"✗\"\n",
    "        print(f\"  {_icon} prompt_{prompt_id} / {alias} ({latency_ms:.0f} ms)\")\n",
    "\n",
    "responses_df = pd.DataFrame(records)\n",
    "print(f\"\\nresponses_df shape: {responses_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6753f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Inspect responses_df ───────────────────────────────────────────────────────\n",
    "pd.set_option(\"display.max_colwidth\", 80)\n",
    "responses_df[[\"prompt_id\", \"model_alias\", \"status\", \"latency_ms\", \"error\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd05362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(model_alias, prompt_id):\n",
    "    \"\"\"Render the raw response for the currently selected model and prompt.\"\"\"\n",
    "    row = responses_df[\n",
    "        (responses_df[\"model_alias\"] == model_alias) &\n",
    "        (responses_df[\"prompt_id\"] == prompt_id)\n",
    "    ]\n",
    "    if row.empty:\n",
    "        text = \"(no response found)\"\n",
    "    else:\n",
    "        text = row.iloc[0][\"response_raw\"]\n",
    "\n",
    "    prompt_preview = prompts[prompt_id][:80].replace(\"\\n\", \" \")\n",
    "    if len(prompts[prompt_id]) > 80:\n",
    "        prompt_preview += \"…\"\n",
    "\n",
    "    display(Markdown(\n",
    "        f\"### Response\\n\"\n",
    "        f\"**LLM:** `{model_alias}`  \\n\"\n",
    "        f\"**Prompt:** `{prompt_preview}`\\n\\n\"\n",
    "        f\"{text}\"\n",
    "    ))\n",
    "\n",
    "\n",
    "model_dd = widgets.Dropdown(\n",
    "    options=generator_aliases,\n",
    "    description=\"LLM:\",\n",
    ")\n",
    "prompt_dd = widgets.Dropdown(\n",
    "    options=[(f\"prompt_{i}\", i) for i in sorted(responses_df[\"prompt_id\"].unique())],\n",
    "    description=\"Prompt:\",\n",
    ")\n",
    "widgets.interact(render, model_alias=model_dd, prompt_id=prompt_dd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5046328c",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Embeddings → Cosine Similarity → Aggregate Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d888f1a2",
   "metadata": {},
   "source": [
    "### 4.1 Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206d0bb5",
   "metadata": {},
   "source": [
    "Now that we have our responses, we need to embed them, and perform the cosine similarity between them in order to numerically compare the differences between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── FR-7: Embed normalized responses & compute per-prompt cosine matrices ──────\n",
    "embedder_alias = next(\n",
    "    (a for a, c in MODELS.items() if c[\"kind\"] == \"embedder\"), None\n",
    ")\n",
    "if embedder_alias is None:\n",
    "    raise ValueError(\n",
    "        \"No embedder model configured. Add an entry with kind='embedder' to LLM_MODELS_JSON.\"\n",
    "    )\n",
    "\n",
    "ok_df = responses_df[responses_df[\"status\"] == \"ok\"].copy()\n",
    "\n",
    "print(f\"Embedding {len(ok_df)} response(s) via '{embedder_alias}' …\")\n",
    "t0_embed = time.time()\n",
    "_embeddings = client.embed(embedder_alias, ok_df[\"response_norm\"].tolist())\n",
    "print(f\"Done in {(time.time() - t0_embed)*1000:.0f} ms.\")\n",
    "\n",
    "ok_df = ok_df.copy()\n",
    "ok_df[\"embedding\"] = _embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e63f6f2",
   "metadata": {},
   "source": [
    "### 4.2 Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44a3c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a: list[float], b: list[float]) -> float:\n",
    "    \"\"\"Cosine similarity between two vectors; returns 0.0 if either is zero-norm.\n",
    "    Args:\n",
    "        a: First vector as a list of floats.\n",
    "        b: Second vector as a list of floats.\n",
    "    Returns:\n",
    "        Cosine similarity as a float in the range [-1.0, 1.0].\n",
    "    \"\"\"\n",
    "    va, vb = np.asarray(a, dtype=float), np.asarray(b, dtype=float)\n",
    "    denom = np.linalg.norm(va) * np.linalg.norm(vb)\n",
    "    return float(np.dot(va, vb) / denom) if denom > 0.0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c085bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = len(generator_aliases)\n",
    "per_prompt_cosine: dict[int, np.ndarray] = {}\n",
    "\n",
    "# Compute cosine similarity matrices for each prompt\n",
    "for pid in sorted(ok_df[\"prompt_id\"].unique()):\n",
    "    sub = ok_df[ok_df[\"prompt_id\"] == pid].set_index(\"model_alias\")\n",
    "    mat = np.full((M, M), np.nan)\n",
    "    for i, a1 in enumerate(generator_aliases):\n",
    "        for j, a2 in enumerate(generator_aliases):\n",
    "            if a1 in sub.index and a2 in sub.index:\n",
    "                mat[i, j] = cosine_sim(\n",
    "                    sub.loc[a1, \"embedding\"], sub.loc[a2, \"embedding\"]\n",
    "                )\n",
    "    per_prompt_cosine[pid] = mat\n",
    "\n",
    "# Aggregate: mean across prompts (ignoring NaN)\n",
    "agg_cosine = np.nanmean(np.stack(list(per_prompt_cosine.values())), axis=0)\n",
    "print(\"\\nAggregate cosine matrix:\")\n",
    "print(pd.DataFrame(agg_cosine, index=generator_aliases, columns=generator_aliases).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a168e3af",
   "metadata": {},
   "source": [
    "### 4.3 Aggregate cosine heatmap between models (mean across prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00167e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── FR-10: Aggregate cosine heatmap ───────────────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(max(4, M + 1), max(3, M)))\n",
    "im = ax.imshow(agg_cosine, vmin=0.0, vmax=1.0, cmap=\"YlGnBu\")\n",
    "ax.set_xticks(range(M))\n",
    "ax.set_xticklabels(generator_aliases, rotation=45, ha=\"right\")\n",
    "ax.set_yticks(range(M))\n",
    "ax.set_yticklabels(generator_aliases)\n",
    "ax.set_title(\"Aggregate Cosine Similarity\\n(mean across prompts)\")\n",
    "plt.colorbar(im, ax=ax, label=\"cosine similarity\")\n",
    "for i in range(M):\n",
    "    for j in range(M):\n",
    "        if not np.isnan(agg_cosine[i, j]):\n",
    "            ax.text(\n",
    "                j, i,\n",
    "                f\"{agg_cosine[i, j]:.2f}\",\n",
    "                ha=\"center\", va=\"center\",\n",
    "                fontsize=9,\n",
    "                color=\"black\" if agg_cosine[i, j] < 0.7 else \"white\",\n",
    "            )\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bc571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cosine_heatmap_for_prompt(prompt_index: int) -> None:\n",
    "    \"\"\"Plot the cosine heatmap for a specific prompt by index.\n",
    "    Args:\n",
    "        prompt_index: The index of the prompt to plot (0-based position in sorted prompt_ids).\n",
    "    Returns:\n",
    "        None. Displays a heatmap of cosine similarities for the specified prompt.\n",
    "    \"\"\"\n",
    "    sorted_pids = sorted(ok_df[\"prompt_id\"].unique())\n",
    "    pid = sorted_pids[prompt_index]\n",
    "    mat = per_prompt_cosine[pid]\n",
    "    fig, ax = plt.subplots(figsize=(max(4, M + 1), max(3, M)))\n",
    "    im = ax.imshow(mat, vmin=0.0, vmax=1.0, cmap=\"YlGnBu\")\n",
    "    ax.set_xticks(range(M))\n",
    "    ax.set_xticklabels(generator_aliases, rotation=45, ha=\"right\")\n",
    "    ax.set_yticks(range(M))\n",
    "    ax.set_yticklabels(generator_aliases)\n",
    "    ax.set_title(f\"Cosine Similarity for prompt_{pid}\")\n",
    "    plt.colorbar(im, ax=ax, label=\"cosine similarity\")\n",
    "    for i in range(M):\n",
    "        for j in range(M):\n",
    "            if not np.isnan(mat[i, j]):\n",
    "                ax.text(\n",
    "                    j, i,\n",
    "                    f\"{mat[i, j]:.2f}\",\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    fontsize=9,\n",
    "                    color=\"black\" if mat[i, j] < 0.7 else \"white\",\n",
    "                )\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c318b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the cosine heatmap for the last prompt only\n",
    "plot_cosine_heatmap_for_prompt(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc7cb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cosine_heatmap_for_prompt(prompt_index: int) -> None:\n",
    "    sorted_pids = sorted(ok_df[\"prompt_id\"].unique())\n",
    "    pid = sorted_pids[prompt_index]\n",
    "    mat = per_prompt_cosine[pid]\n",
    "\n",
    "    # Get a preview of the prompt text for the title (first 60 chars, single line)\n",
    "    prompt_preview = prompts[pid][:60].replace(\"\\n\", \" \")\n",
    "    if len(prompts[pid]) > 60:\n",
    "        prompt_preview += \"…\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(max(4, M + 1), max(3, M)))\n",
    "    im = ax.imshow(mat, vmin=0.0, vmax=1.0, cmap=\"YlGnBu\")\n",
    "    ax.set_xticks(range(M))\n",
    "    ax.set_xticklabels(generator_aliases, rotation=45, ha=\"right\")\n",
    "    ax.set_yticks(range(M))\n",
    "    ax.set_yticklabels(generator_aliases)\n",
    "    fig.suptitle(f\"Cosine Similarity for prompt_{pid}\", fontsize=11, y=1.0)\n",
    "    ax.set_title(f'\"{prompt_preview}\"', fontsize=10, color=\"gray\", style=\"italic\", pad=4)\n",
    "    plt.colorbar(im, ax=ax, label=\"cosine similarity\")\n",
    "    for i in range(M):\n",
    "        for j in range(M):\n",
    "            if not np.isnan(mat[i, j]):\n",
    "                ax.text(\n",
    "                    j, i,\n",
    "                    f\"{mat[i, j]:.2f}\",\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    fontsize=9,\n",
    "                    color=\"black\" if mat[i, j] < 0.7 else \"white\",\n",
    "                )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# ── Interactive dropdown ───────────────────────────────────────────────────────\n",
    "sorted_pids = sorted(ok_df[\"prompt_id\"].unique())\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=[(f\"prompt_{pid}\", i) for i, pid in enumerate(sorted_pids)],\n",
    "    description=\"Prompt:\",\n",
    ")\n",
    "widgets.interact(plot_cosine_heatmap_for_prompt, prompt_index=dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9f92f5",
   "metadata": {},
   "source": [
    "### 4.4 Aggregate cosine heatmap between prompts (mean across models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ffd82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Compute prompt × prompt cosine matrices per model, aggregate across models ─\n",
    "prompt_ids    = sorted(ok_df[\"prompt_id\"].unique())\n",
    "N             = len(prompt_ids)\n",
    "pid_to_idx    = {pid: idx for idx, pid in enumerate(prompt_ids)}\n",
    "prompt_labels = [f\"prompt_{pid}\" for pid in prompt_ids]\n",
    "\n",
    "per_model_prompt_cosine: dict[str, np.ndarray] = {}\n",
    "\n",
    "for alias in generator_aliases:\n",
    "    sub   = ok_df[ok_df[\"model_alias\"] == alias].set_index(\"prompt_id\")\n",
    "    mat   = np.full((N, N), np.nan)\n",
    "    for i, pi in enumerate(prompt_ids):\n",
    "        for j, pj in enumerate(prompt_ids):\n",
    "            if pi in sub.index and pj in sub.index:\n",
    "                mat[i, j] = cosine_sim(\n",
    "                    sub.loc[pi, \"embedding\"], sub.loc[pj, \"embedding\"]\n",
    "                )\n",
    "    per_model_prompt_cosine[alias] = mat\n",
    "\n",
    "# Aggregate: mean across models (ignoring NaN)\n",
    "agg_prompt_cosine = np.nanmean(np.stack(list(per_model_prompt_cosine.values())), axis=0)\n",
    "print(\"\\nAggregate cosine matrix (prompt × prompt, mean across models):\")\n",
    "print(pd.DataFrame(agg_prompt_cosine, index=prompt_labels, columns=prompt_labels).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b73738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Prompt × prompt cosine heatmap (mean across models) ───────────────────────\n",
    "fig, ax = plt.subplots(figsize=(max(4, N + 1), max(3, N)))\n",
    "im = ax.imshow(agg_prompt_cosine, vmin=0.0, vmax=1.0, cmap=\"YlGnBu\")\n",
    "ax.set_xticks(range(N))\n",
    "ax.set_xticklabels(prompt_labels, rotation=45, ha=\"right\")\n",
    "ax.set_yticks(range(N))\n",
    "ax.set_yticklabels(prompt_labels)\n",
    "ax.set_title(f\"Prompt × Prompt Cosine Similarity\\n(mean across {len(generator_aliases)} model(s))\")\n",
    "plt.colorbar(im, ax=ax, label=\"cosine similarity\")\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        if not np.isnan(agg_prompt_cosine[i, j]):\n",
    "            ax.text(\n",
    "                j, i,\n",
    "                f\"{agg_prompt_cosine[i, j]:.2f}\",\n",
    "                ha=\"center\", va=\"center\",\n",
    "                fontsize=9,\n",
    "                color=\"black\" if agg_prompt_cosine[i, j] < 0.7 else \"white\",\n",
    "            )\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec7ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt × prompt cosine heatmaps per model (interactive dropdown) ─────────────────\n",
    "def plot_prompt_cosine_heatmap_for_model(model_alias: str) -> None:\n",
    "    \"\"\"Plot the prompt × prompt cosine heatmap for a specific model alias.\n",
    "    Args:\n",
    "        model_alias: The model alias to plot (must be one of the generator aliases).\n",
    "    Returns:\n",
    "        None. Displays a heatmap of cosine similarities for the specified model.\n",
    "    \"\"\"\n",
    "    mat = per_model_prompt_cosine[model_alias]\n",
    "    fig, ax = plt.subplots(figsize=(max(4, N + 1), max(3, N)))\n",
    "    im = ax.imshow(mat, vmin=0.0, vmax=1.0, cmap=\"YlGnBu\")\n",
    "    ax.set_xticks(range(N))\n",
    "    ax.set_xticklabels(prompt_labels, rotation=45, ha=\"right\")\n",
    "    ax.set_yticks(range(N))\n",
    "    ax.set_yticklabels(prompt_labels)\n",
    "    ax.set_title(f\"Prompt × Prompt Cosine Similarity for {model_alias}\")\n",
    "    plt.colorbar(im, ax=ax, label=\"cosine similarity\")\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if not np.isnan(mat[i, j]):\n",
    "                ax.text(\n",
    "                    j, i,\n",
    "                    f\"{mat[i, j]:.2f}\",\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    fontsize=9,\n",
    "                    color=\"black\" if mat[i, j] < 0.7 else \"white\",\n",
    "                )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)  # prevent duplicate renders in VS Code\n",
    "\n",
    "\n",
    "# ── Interactive dropdown ───────────────────────────────────────────────────────\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=generator_aliases,\n",
    "    description=\"Model:\",\n",
    ")\n",
    "widgets.interact(plot_prompt_cosine_heatmap_for_model, model_alias=dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff730331",
   "metadata": {},
   "source": [
    "## 5. Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24199dc8",
   "metadata": {},
   "source": [
    "- Use better semantic similarity metrics.\n",
    "- Have another LLM be the semantic similarity judge.\n",
    "- Use a frontier reasoning model to act as a 'base truth'/benchmark against the smaller models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
